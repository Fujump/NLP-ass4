{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamaapi import LlamaAPI\n",
    "\n",
    "# Replace 'Your_API_Token' with your actual API token\n",
    "llama = LlamaAPI(\"LL-thOwDPY4UohvmFVbrbHMTAgAV58hzRQMVLivrl89xXMFtEJNI8kcSOdJhO2Ppdln\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms import ChatLlamaAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatLlamaAPI(client=llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3501405]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "|  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "|  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "|  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "|  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "|_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/chat_llm_chain/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /chat_llm_chain/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "\n",
      "\u001b[1;31;40mLANGSERVE:\u001b[0m ⚠️ Using pydantic 2.5.2. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human.\n",
      "Human: remember that I have a black cap.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "INFO:     127.0.0.1:35824 - \"POST /chat_llm_chain/invoke HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human.\n",
      "Human: remember that I have a black cap.\n",
      "AI: 🎩 waves Hello! How may I assist you today? Oh, and I see you're wearing a black cap. That's a stylish touch! 😎 Do you have any specific questions or topics you'd like to discuss?\n",
      "Human: what's the color of my cap?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "INFO:     127.0.0.1:53888 - \"POST /chat_llm_chain/invoke HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human.\n",
      "Human: remember that I have a black cap.\n",
      "AI: 🎩 waves Hello! How may I assist you today? Oh, and I see you're wearing a black cap. That's a stylish touch! 😎 Do you have any specific questions or topics you'd like to discuss?\n",
      "Human: what's the color of my cap?\n",
      "AI: 👀 Oh my, you\"re asking me to describe the color of your cap? 🤔 Let me think... 🎨 Hmm, I would say your cap is... (drumroll please)... BLACK! 😱 Did I get it right? 😄\n",
      "Human: Have I told you about that?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "INFO:     127.0.0.1:58092 - \"POST /chat_llm_chain/invoke HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human.\n",
      "Human: remember that I have a black cap.\n",
      "AI: 🎩 waves Hello! How may I assist you today? Oh, and I see you're wearing a black cap. That's a stylish touch! 😎 Do you have any specific questions or topics you'd like to discuss?\n",
      "Human: what's the color of my cap?\n",
      "AI: 👀 Oh my, you\"re asking me to describe the color of your cap? 🤔 Let me think... 🎨 Hmm, I would say your cap is... (drumroll please)... BLACK! 😱 Did I get it right? 😄\n",
      "Human: Have I told you about that?\n",
      "AI: 😳 Oops, it seems I may have forgotten! 🙈 Do tell, when did you mention your black cap? Was it just now? 🤷‍♀️ Oh no, maybe I\"ve gone and forgotten again! 😂\n",
      "Human: tell a brief story.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "INFO:     127.0.0.1:49620 - \"POST /chat_llm_chain/invoke HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human.\n",
      "Human: remember that I have a black cap.\n",
      "AI: 🎩 waves Hello! How may I assist you today? Oh, and I see you're wearing a black cap. That's a stylish touch! 😎 Do you have any specific questions or topics you'd like to discuss?\n",
      "Human: what's the color of my cap?\n",
      "AI: 👀 Oh my, you\"re asking me to describe the color of your cap? 🤔 Let me think... 🎨 Hmm, I would say your cap is... (drumroll please)... BLACK! 😱 Did I get it right? 😄\n",
      "Human: Have I told you about that?\n",
      "AI: 😳 Oops, it seems I may have forgotten! 🙈 Do tell, when did you mention your black cap? Was it just now? 🤷‍♀️ Oh no, maybe I\"ve gone and forgotten again! 😂\n",
      "Human: tell a brief story.\n",
      "AI: 📚 Ooh, I love a good story! 😍 Let me think... 🤔 Once upon a time, in a land far, far away... 🏰 Wait, what was that? 🐰 Oh right, I was about to tell a story! 😅 Okay, so imagine a magical kingdom where the skies are always gray and the clouds are shaped like... (drumroll please)... CAPS! 😱 The kingdom\"s ruler, the wise and just King Cap, has declared that all citizens must wear caps of their choice to show their individuality and creativity! 🎉 The end. 😊 What did you think? Did I do a good job? 😅\n",
      "Human: why did the king say that?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "INFO:     127.0.0.1:42676 - \"POST /chat_llm_chain/invoke HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [3501405]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langserve import add_routes\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a chatbot having a conversation with a human.\"\n",
    "        ),  # The persistent system prompt\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),  # Where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),  # Where the human input will injected\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chat_llm_chain = LLMChain(\n",
    "    llm=model,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "# 2. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 3. Adding chain route\n",
    "add_routes(\n",
    "    app,\n",
    "    chat_llm_chain,\n",
    "    path=\"/chat_llm_chain\",\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('huq': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26d6c7b4ea4c17c1a704ac8c3dabc344c5e691f1557a5e845366920aa078ec69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
